{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Session: Write-along introduction to Deep Learning\n",
    "\n",
    "Lab objectives:\n",
    "- develop intuition for Deep Learning (DL) \n",
    "- introduction to PyTorch\n",
    "- core concepts of training Deep Neural Networks in practice.\n",
    "\n",
    "Lab format\n",
    "- We will be doing this lab session **all together**: I will tell you what to write in the code as we move forward. \n",
    "- **The core idea of the lab is to use the whole class to search for hyperparameters** as a mini data center. We will all choose different parameters, which will give us a broad view of what works best for this problem. Make sure you follow along! \n",
    "- Write in all the values you find in [this spreasheet](https://docs.google.com/spreadsheets/d/1BCO2RxfEHnIgVNN7mIvxQ7aiU9aMAXTYl-wDZg2y0l0/edit?usp=sharing).\n",
    "- This lab is *not graded*. We still require your presence so we can run all the experiments together in parallel. You won't need anything but the notebook and an environment to run it. **You don't need ChatGPT open for anything!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Loading the Data: the classic MNIST dataset\n",
    "The dataset we will use is MNIST, consisting of 60k hand-written digits; the goal is to tell which number is on the image, or in other words to \"read\" hand-written digits. There are 10k testing samples in addition to the 60k training samples.\n",
    "\n",
    "We will be using [this tuto](https://medium.com/@nutanbhogendrasharma/pytorch-convolutional-neural-network-with-mnist-dataset-4e8a4265e118) as a guide for most of the prep work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is split into 50k elements in `train_data`, which we will use to train the network with backprop, and 10k elements in `test_data`, which we will use as never-seen digits to test the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,                         \n",
    "    transform = ToTensor(), \n",
    "    download = True,         \n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root = 'data', \n",
    "    train = False, \n",
    "    transform = ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how these look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5,5, figsize=(10,10))\n",
    "for i,ax in enumerate (axs.flatten()):\n",
    "    ax.imshow(train_data.data[i], cmap='gray') # visualize the image\n",
    "    ax.set_title(train_data.targets[i].item()) # what it was supposed to be\n",
    "    ax.set_axis_off()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the shape of these images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Image shape: {}\".format(train_data.data[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will want to sample from the test or train set during training and testing. In order to do this automatically, we use DataLoaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "loaders = {\n",
    "    # TODO\n",
    "    # TODO\n",
    "}\n",
    "loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can loop over the loaders in batches using a for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in loaders['test']:\n",
    "    print(image.shape, label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch treats images with the number of *channels*, mainly 3 channels for RGB. In our case, we have grayscale images, n_channels=1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Fully Connected Network (aka MLP, Multi Layer Perceptron)\n",
    "\n",
    "A *Fully Connected* or *Multilayer Perceptron* (MLP) is the most elementary class of feed forward artificial neural network (ANN).\n",
    "\n",
    "<!-- ![image.png](attachment:image.png)\n",
    " -->\n",
    "<img src=https://scikit-learn.org/stable/_images/multilayerperceptron_network.png alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "[Image source](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)\n",
    "\n",
    "The core operation of the Fully Connected Network is to apply a linear mapping (like a linear regression) with parameter matrix $W$ and bias vector $b$, followed by an activation function $\\sigma$:\n",
    "\n",
    "$$ l_i (x) = \\sigma (Wx + b) $$\n",
    "\n",
    "This is a single \"layer\" $l_i$; the \"multi-layer\" network chains these operations together: \n",
    "\n",
    "$$ f_\\theta(x) = l_d (l_{d-1}( \\cdots l_2( l_1 (x))\\cdots))$$\n",
    "\n",
    " The above image shows a one-hidden-layer fully connected network with $n$ inputs and scalar output (1D vector); mathematically, this network with weights called $\\theta$ is a function $f_\\theta: \\mathbb{R}^n \\rightarrow \\mathbb{R}$.\n",
    "\n",
    "\n",
    "Most of what we will do from now on can be visualized beforehand on a toy problem using the [Tensorflow Playground](https://playground.tensorflow.org/#activation=sigmoid&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=2&seed=0.35230&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false).\n",
    "On this playground, you can change the inputs, depth and width of the network, learning rate, task and noise, activation function, regularizer, batch size. We very strongly recommend you give it some time to play around, as it is excellent to build an intuition and understand the impact of all the choices the NN designer makes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network architecture\n",
    "Three main choices can be made about the architecture of a fully connected network:\n",
    "* *depth*: number of hidden layers\n",
    "* *width*: number of neurons in each layer\n",
    "* *non-linearity*: activation functions\n",
    "\n",
    "Using only one hidden layer, but with sufficient width, ensures that the network is a **Universal Approximator** (can approximate any function $g: \\mathbb{R}^n \\rightarrow \\mathbb{R}$). However, the success of Deep Learning comes from understanding that deeper (more layers) is generally better than wider (more neurons per layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    # define nn\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.in_dim = 28*28\n",
    "        self.out_dim = 10\n",
    "\n",
    "        # TODO\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = X.squeeze(1) # remove the \"channel\" dimension (1)\n",
    "        X = X.flatten(-2,-1) # convert the picture to a single vector\n",
    "\n",
    "        # TODO\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try passing the very first element of the dataset in the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = train_data.data[0]/255.\n",
    "net = Net()\n",
    "prediction_s0 = net(s0)\n",
    "prediction_s0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, these don't correspond to probabilities at all. Depending on the loss function we want to use, we will have to adapt how to interpret the output.\n",
    "\n",
    "Let's see how to train this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Training Loop\n",
    "\n",
    "In order to make experiments scientifically relevant, we need to be able to reset the network and optimizer before each experiment. A useful little function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for convenience\n",
    "def reset(net_class, criterion_class, optim_class, lr):\n",
    "    net = # TODO\n",
    "    optimizer = # TODO\n",
    "    criterion = criterion_class()\n",
    "    return net, criterion, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the main training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.autograd import Variable \n",
    "total_step = len(loaders['train'])\n",
    "\n",
    "def train(num_epochs, loaders, net, optimizer, criterion):\n",
    "    # Visualization stuff\n",
    "    logs = {'train_losses':[], 'test_losses':[], 'accuracies':[], 'steps':[], 'test_steps':[]}\n",
    "    step = 0\n",
    "    \n",
    "    # Learning stuff. Set the net to training mode:\n",
    "    net.train()\n",
    "    # Learning loop:\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']): # extracts batches of data using loaders\n",
    "            # Compute loss\n",
    "            # TODO\n",
    "\n",
    "            # Compute gradient (backprop) and apply SGD\n",
    "            # TODO\n",
    "            \n",
    "            step += 1\n",
    "\n",
    "            # logging of training stuff\n",
    "            logs['train_losses'].append(loss.item())\n",
    "            logs['steps'].append(step) \n",
    "            \n",
    "            # Testing and logging\n",
    "            if ((i+1) % 200 == 0) or step==1 : # perform test step\n",
    "                test_update_logs(logs, net, criterion, i, step, epoch, num_epochs)\n",
    "\n",
    "    return logs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_update_logs(logs, net, criterion, i, step, epoch, num_epochs):\n",
    "    test_loss, acc = test(net, criterion)\n",
    "    print ('Epoch [{}/{}], Step [{}/{}], Test Loss: {:.4f}, Test Accuracy: {:.4f}' \n",
    "            .format(epoch + 1, num_epochs, i + 1, total_step, test_loss, acc))\n",
    "    logs['test_losses'].append(test_loss)\n",
    "    logs['accuracies'].append(acc)\n",
    "    logs['test_steps'].append(step)\n",
    "    if ((i+1)%600)==0:\n",
    "        plot(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, criterion):\n",
    "    # Test the model\n",
    "    net.eval()    \n",
    "    with torch.no_grad():\n",
    "        total_loss = []\n",
    "        total_acc = []\n",
    "        for images, labels in loaders['test']:\n",
    "            test_output = net(images)\n",
    "            labels_onehot = F.one_hot(labels, num_classes=10)*1.\n",
    "            test_loss = criterion(test_output, labels_onehot)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "            accuracy = (pred_y == labels).sum().item() / float(labels.size(0))\n",
    "\n",
    "            total_loss.append(test_loss)\n",
    "            total_acc.append(accuracy)\n",
    "    total_loss = np.mean(total_loss)\n",
    "    total_acc = np.mean(total_acc)\n",
    "    return total_loss, total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(logs):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    ax1.plot(logs['steps'], logs['train_losses'], label=\"train loss\")\n",
    "    ax1.plot(logs['test_steps'], logs['test_losses'], label=\"test loss\")\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_title(\"Losses (log scale)\")\n",
    "    ax1.legend()\n",
    "    ax2.plot(logs['test_steps'], logs['accuracies'])\n",
    "    ax2.set_title(\"Accuracy\")\n",
    "    ax2.set_ylim(0,1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter box - change things here to launch a new run!\n",
    "HIDDEN_WIDTH = # TODO\n",
    "ACTIVATION = # TODO\n",
    "LEARNING_RATE = # TODO\n",
    "OPTIMIZER = # TODO\n",
    "CRITERION = nn.MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net, criterion, optimizer = reset(Net, CRITERION, OPTIMIZER, lr=LEARNING_RATE)\n",
    "logs_base = train(1, loaders, net, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Run Average Accuracy {np.mean(logs_base['accuracies']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course reminders for the different hyperparameters and architectural choices\n",
    "If you haven't heard about some of these yet, it just means we'll cover it in the next session!\n",
    "\n",
    "### 1. Architecture\n",
    "The architecture of the network is the backbone of our function - what type of computations it does. \n",
    "\n",
    "Three main choices can be made about the architecture of a fully connected network:\n",
    "\n",
    "- **Depth**: the number of hidden layers; from an intuitive perspective, the number of layers of \"abtraction\" that the network learns. Deeper networks might lead to gradient issues without careful management of the architecture and learning algorithm. Increasing the depth linearly increases parameter count, and therefore inference and backprop times.\n",
    "\n",
    "- **Width**: number of neurons in each layer; from the same intuitive perspective, how many components/dimension in each abstraction. Increasing the width quadratically increases parameter count, and therefore inference and backprop times.\n",
    "\n",
    "- **Activation function** / **Non-linearity**: How the network will \"split\" the space at each layer. This is critical to enable universal approximation, but not all activations are equal in practice. The sigmoid, while important from a historic perspective, has issues with its gradient flow, and is now completely obsolete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2. Gradient Descent\n",
    "\n",
    "Gradient descent follows the opposite direction of the gradient of the loss, \n",
    "$$ \\theta_i \\leftarrow \\theta_i + \\eta \\frac{\\partial \\mathcal{L}}{\\partial \\theta_i}. $$\n",
    "\n",
    "There are a bunch of elements that we can play around with in this formula.\n",
    "\n",
    "- **Learning Rate**: The learning rate $\\eta$ of the gradient descent is a very common and critical hyperparameter in a lot of ML applications, including Deep Learning. <br> <br>\n",
    "Generally, the learning rate needs to go down when the network gets bigger (finer steps to take), and up when we use bigger batches (more confident gradients). It's quite hard to get a good feel for it, trial and error is necessary. Nowadays, default hyperparameters often work decently well.\n",
    "\n",
    "- **Loss**:  The loss $\\mathcal{L}$ expresses our objective, what we want the network to accomplish (minimize). It is generally some version of \"get as close to the ideal function $g(x)=y$\", when we only have access to some examples of $y$ (data points). <br>\n",
    "The most intuitive loss to optimize is simply the Mean Squared Error, $$\\mathcal{L}_{\\text{MSE}}=\\sum_i(f_\\theta(x)_i - y_i)^2,$$ trying to minimize the squared distance between your prediction and the ground truth. Other similar losses exist, we will try MAE $\\mathcal{L}_{\\text{MAE}}=\\sum_i \\left|f_\\theta(x)_i - y_i\\right|$ and Huber (combining MSE when higher than 1 and MAE when smaller). <br> <br>\n",
    "But mainly, we will try CrossEntropy, the main loss for doing classification: $$\\mathcal{L}_{\\text{CE}}=-\\sum_i y_i \\log f_\\theta(x)_i.$$ For one-hot labels (like our MNIST case here: the digit cannot be 80% a `5` and 20% a `3`, there has to be a single answer), this reduces to $\\mathcal{L}_{\\text{CE}}=- \\log f_\\theta(x)_c $ for $c$ the class we should have predicted. In other words: make that class as likely (prediction as high) as possible!\n",
    "\n",
    "- **Optimizer**\n",
    "Computing $\\partial \\mathcal{L} / \\partial \\theta_i$ over the entire dataset is expensive or sometimes impossible (think ChatGPT with teras of internet text data). Instead we compute it over mini-batches of data, $\\partial \\mathcal{L}_{mb} / \\partial \\theta_i$. This means the loss and gradient are now approximations, with noise due to the random sampling of the data: using this to update $\\theta_i$ is generally called *Stochastic Gradient Descent*. \n",
    "<br> <br>\n",
    "We can come up with smarter ways of using our approximate gradient, such as using momentum or adaptive learning rates per-parameter $\\theta_i$. We then obtain different *optimizers*, different methods to change $\\theta_i$ from a given approximate gradient. SGD is the basis, other algorithms came up later, with Adam taking the throne as most popular.\n",
    "\n",
    "- **Batch size**\n",
    "The reason the optimizer is called Stochastic Gradient Descent, as opposed to Gradient Descent, is because we only use subsets (batches) of the training data instead of the whole thing at once, acting like a sample in a stochastic computation. This was found to lead to great gains in wall-clock performance, since we don't have to loop over the whole dataset, which might be millions of entries big. In particular, this has lead to huge gains in efficiency thanks to GPUs, massively excelling in parallelized computing but with limited RAM that cannot hold the whole dataset at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Regularization\n",
    "- **Weight decay**:  A very common cause for overfitting is that the network weights explode - if you try to fit 10 2D points with a 10 degree polynomial, you will often find very high weight values that lead to severe overfitting, rather than truly trying to find the trend.\n",
    "In order to prevent weight explosion, *L2 Regularization* add a soft constraint to the loss under the form of a $\\lambda||w||_2$ term (L1 Reg uses norm 1). This way, the optimizer tries to solve the task using weights as small as possible. Conveniently in PyTorch, [as you can see in the doc](https://docs.pytorch.org/docs/stable/optim.html#per-parameter-options), the Regularization (\"weight decay\") is an optional argument to the optimizer!\n",
    "<br> <br>\n",
    "`optimizer = torch.optim.SGD(net.parameters(), lr=0.01, weight_decay=0.01)`.\n",
    "<br>\n",
    "If you want to use it with Adam though, use `AdamW` as an optimizer instead.\n",
    "\n",
    "\n",
    "- **Dropout** The neurons of a neural network are extremely heavily dependent on the values of the previous neurons - each of the inputs can have a drastic impact on the output. This is often a major culprit for overfitting, where the neurons cannot generalize properly because the new testing distribution looks very different from the training distribution.\n",
    "<br> <br>\n",
    "In order to prevent these heavy dependencies, Dropout was introduced. In training, each neuron has some probability to be turned off altogether! This means that the downstream neurons need to be flexible enough to adapt to all kinds of changes in input; no rely too heavily on a single input, but rather find valuable information in all of it.\n",
    "Dropout can conveniently be seen as an [additional layer](https://pytorch.org/docs/stable/nn.html#dropout-layers), that you can add after any layer (except the output), with a constant giving the probability to turn the neuron off.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
